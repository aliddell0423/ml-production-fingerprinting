{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import hdbscan\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "DATA = Path(\"./data\")\n",
    "RAW = DATA / \"raw\"\n",
    "PROC = DATA / \"processed\"\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 1337\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "wo = pd.read_csv(RAW / \"work_orders.csv\")\n",
    "logs = pd.read_csv(RAW / \"logs.csv\")\n",
    "stop = pd.read_csv(RAW / \"stopworks.csv\")\n",
    "\n",
    "# aggregate logs per WO into a single document\n",
    "doc = (logs\n",
    "       .sort_values([\"work_order_id\",\"ts\"])\n",
    "       .groupby(\"work_order_id\")[\"message\"]\n",
    "       .apply(lambda s: \" \\n\".join(s.astype(str).tolist()))\n",
    "       .reset_index()\n",
    "       .rename(columns={\"message\":\"log_doc\"}))\n",
    "\n",
    "df = wo.merge(doc, on=\"work_order_id\", how=\"left\")\n",
    "df[\"log_doc\"] = df[\"log_doc\"].fillna(\"\")\n",
    "df.head(3)\n"
   ],
   "id": "b0abdf756d4b0d9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,3),\n",
    "    min_df=5,           # adjust based on corpus size\n",
    "    max_df=0.6,\n",
    "    strip_accents=\"unicode\",\n",
    "    lowercase=True\n",
    ")\n",
    "X_text = tfidf.fit_transform(df[\"log_doc\"])\n",
    "with open(PROC / \"log_vocab.json\", \"w\") as f:\n",
    "    json.dump(tfidf.vocabulary_, f)\n",
    "sparse.save_npz(PROC / \"log_tfidf_sparse.npz\", X_text)\n",
    "X_text.shape\n"
   ],
   "id": "17057cf08f3f67c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HDBSCAN prefers a distance matrix or a metric; with sparse TFIDF we use cosine metric.\n",
    "# Using min_cluster_size ~ 25-60 is reasonable; tune as needed.\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=40,\n",
    "    min_samples=None,\n",
    "    metric='cosine',\n",
    "    cluster_selection_epsilon=0.0,\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True # enables membership/probabilities\n",
    ")\n",
    "\n",
    "labels = clusterer.fit_predict(X_text)\n",
    "prob = getattr(clusterer, 'probabilities_', np.ones_like(labels, dtype=float))\n",
    "df[\"cluster_id\"] = labels\n",
    "df[\"cluster_conf\"] = prob\n",
    "df[\"is_noise\"] = (labels == -1).astype(int)\n",
    "\n",
    "# optional: soft clustering features via approximate prediction\n",
    "# from hdbscan.prediction import membership_vector\n",
    "# soft_membership = membership_vector(clusterer, X_text)  # dict-like per row\n",
    "df[\"cluster_id\"].value_counts().head(10)\n"
   ],
   "id": "c667d719dc8e0126"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# What words define top clusters?\n",
    "inv_vocab = {v:k for k,v in tfidf.vocabulary_.items()}\n",
    "def top_terms_for_cluster(cid, topk=15):\n",
    "    idx = np.where(df[\"cluster_id\"].values == cid)[0]\n",
    "    if len(idx) == 0: return []\n",
    "    mean_vec = X_text[idx].mean(axis=0).A1\n",
    "    top = mean_vec.argsort()[::-1][:topk]\n",
    "    return [inv_vocab[i] for i in top]\n",
    "\n",
    "for cid in df[\"cluster_id\"].value_counts().index[:5]:\n",
    "    if cid == -1: continue\n",
    "    print(f\"Cluster {cid}: \", top_terms_for_cluster(cid))\n"
   ],
   "id": "f2330b40786c51f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Optional: bring in normalized Stopworks labels as features\n",
    "stop_feats = stop.copy()\n",
    "stop_feats[\"has_stopworks\"] = 1\n",
    "stop_feats[\"norm_subsystem\"] = stop_feats[\"norm_subsystem\"].replace(\"\", np.nan)\n",
    "stop_feats[\"norm_root_cause\"] = stop_feats[\"norm_root_cause\"].replace(\"\", np.nan)\n",
    "\n",
    "df2 = df.merge(stop_feats[[\"work_order_id\",\"has_stopworks\",\"norm_subsystem\",\"norm_root_cause\"]],\n",
    "               on=\"work_order_id\", how=\"left\")\n",
    "\n",
    "df2[\"has_stopworks\"] = df2[\"has_stopworks\"].fillna(0).astype(int)\n",
    "for c in [\"norm_subsystem\",\"norm_root_cause\"]:\n",
    "    df2[c] = df2[c].fillna(\"UNK\")\n",
    "\n",
    "y = df2[\"failure_label\"].astype(int)\n",
    "\n",
    "cat_cols = [\"catalog_id\",\"supplier\",\"device_type\",\"technician\",\"shift\",\"norm_subsystem\",\"norm_root_cause\"]\n",
    "num_cols = [\"cluster_conf\",\"is_noise\"]\n",
    "ord_cols = []  # add dates if encoding e.g., day-of-week/seasonality\n",
    "\n",
    "X_tab = df2[cat_cols + num_cols + ord_cols + [\"cluster_id\"]].copy()\n",
    "# Turn cluster_id into a categorical string so OneHot can handle -1 nicely\n",
    "X_tab[\"cluster_id\"] = \"cid_\" + X_tab[\"cluster_id\"].astype(str)\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True), cat_cols + [\"cluster_id\"]),\n",
    "        (\"passthru\", \"passthrough\", num_cols + ord_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    oob_score=False\n",
    ")\n",
    "pipe = Pipeline([(\"prep\", pre), (\"rf\", clf)])\n"
   ],
   "id": "d13a0146e38cb4c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tab, y, test_size=0.25, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "proba = pipe.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, pred, digits=3))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, proba))\n"
   ],
   "id": "21e27702d09520d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "no_cluster_cols = cat_cols + num_cols  # drop cluster_id feature\n",
    "X_nc = df2[no_cluster_cols].copy()\n",
    "\n",
    "pre_nc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True), cat_cols),\n",
    "        (\"passthru\", \"passthrough\", num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0\n",
    ")\n",
    "pipe_nc = Pipeline([(\"prep\", pre_nc), (\"rf\", RandomForestClassifier(\n",
    "    n_estimators=400, n_jobs=-1, random_state=SEED, class_weight=\"balanced_subsample\"\n",
    "))])\n",
    "\n",
    "X_train_nc, X_test_nc, y_train_nc, y_test_nc = train_test_split(\n",
    "    X_nc, y, test_size=0.25, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "pipe_nc.fit(X_train_nc, y_train_nc)\n",
    "pred_nc = pipe_nc.predict(X_test_nc)\n",
    "proba_nc = pipe_nc.predict_proba(X_test_nc)[:,1]\n",
    "\n",
    "print(\"== Without clusters ==\")\n",
    "print(classification_report(y_test_nc, pred_nc, digits=3))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_nc, proba_nc))\n"
   ],
   "id": "152571f567befdcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cluster_sizes = df[\"cluster_id\"].value_counts().rename_axis(\"cluster_id\").reset_index(name=\"size\")\n",
    "display(cluster_sizes.head(10))\n",
    "\n",
    "# Failure rate per cluster\n",
    "fr_per_cluster = (\n",
    "    df2.groupby(\"cluster_id\")[\"failure_label\"].mean()\n",
    "      .rename(\"failure_rate\").reset_index()\n",
    "      .merge(cluster_sizes, on=\"cluster_id\", how=\"left\")\n",
    "      .sort_values(\"failure_rate\", ascending=False)\n",
    ")\n",
    "display(fr_per_cluster.head(15))\n"
   ],
   "id": "80b452e440cc80cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
